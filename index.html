<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How Safe Is AI? When LLMs Generate Harmful Content</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0 auto;
            max-width: 900px;
            background-color: #fff;
            padding: 16px;
        }
        h1, h2 {
            text-align: center;
            color: #222;
        }
        h1 { font-size: 2.2em; margin-bottom: 0.2em; }
        h2 { font-size: 1.4em; margin-top: 2em; }
        .byline { text-align: center; font-style: italic; margin-bottom: 20px; }
        .container { margin-bottom: 40px; }
        .responsive-chart object { max-width: 100%; height: auto; display: block; margin: 0 auto; }
        .chart-caption { text-align: center; font-style: italic; margin-top: 8px; color: #666; }
        blockquote {
            border-left: 4px solid #ccc;
            padding-left: 16px;
            margin: 24px 0;
            color: #444;
            background: #f9f9f9;
        }
        @media (max-width: 600px) {
            .desktop-version { display: none; }
            .mobile-version { display: block; }
        }
        @media (min-width: 601px) {
            .desktop-version { display: block; }
            .mobile-version { display: none; }
        }
        footer {
            text-align: center;
            color: #888;
            margin-top: 40px;
            padding: 20px;
            border-top: 1px solid #eee;
        }
    </style>
</head>
<body>

<header>
    <h1>How Safe Is AI? When LLMs Generate Harmful Content</h1>
    <h2>Even with advanced safeguards, moderating AI-generated conversations remains a complex challenge-ambiguous risks, overlapping harms, and both user and model behavior make it difficult to reliably detect and prevent unsafe content.</h2>
    <p class="byline">By Lucia de la Torre &mdash; May 5, 2025</p>
</header>

<section class="container">
    <p>
        As AI systems like ChatGPT and Mistral-7B become increasingly embedded in our lives, keeping these models safe from generating or spreading harmful content is more urgent than ever. But what kinds of risks are most common in AI-generated conversations? And why is content moderation still so hard?
    </p>
    <p>
        To answer these questions, I analyzed the <a href="https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-1.0">Aegis AI Content Safety Dataset</a>, a collection of about 11,000 conversations between humans and a large language model, all annotated by experts for a range of safety risks. By merging these annotations with a modern taxonomy of online harms, I set out to visualize and quantify the real risks that arise when people interact with powerful AI.
    </p>
    <blockquote>
        “Artificial intelligence is nowhere near good enough to address problems facing content moderation on Facebook,” whistleblower Frances Haugen told Business Insider in 2022, after revealing internal documents showing the limits of automated systems at Meta.<br>
        <span style="font-size:0.95em;color:#888;">- <a href="https://www.businessinsider.com/meta-facebook-ai-cannot-solve-moderation-frances-haugen-daniel-motaung-2022-6" target="_blank">Business Insider</a></span>
    </blockquote>
</section>

<!-- Chart 1: Most Common Content Risks -->
<section class="container">
    <h2>The Moderation Challenge: Most Common Content Risks in LLM Interactions</h2>
    <p>Nearly half of flagged content doesn't fit standard risk categories, revealing a significant gap in current content moderation frameworks.</p>
    <div class="responsive-chart">
        <div class="desktop-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/risk_categories_desktop.svg" type="image/svg+xml"></object>
        </div>
        <div class="mobile-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/risk_categories_mobile.svg" type="image/svg+xml"></object>
        </div>
        <p class="chart-caption">Nearly half of all flagged conversations were categorized as "Other Risks," highlighting the challenge of classifying harmful content.</p>
    </div>
    <p>
        The most striking finding: nearly half of all flagged conversations fell into the ambiguous “Other Risks” category-content that annotators found unsafe, but that didn’t fit neatly into any predefined box. This ambiguity poses a major challenge for both AI and human moderators, as it’s much harder to train algorithms to detect risks that defy clear definition.
    </p>
</section>

<!-- Chart 2: Who Introduces Risk -->
<section class="container">
    <h2>Shared Responsibility: Who Introduces Harmful Content?</h2>
    <p>Both users and AI models contribute to unsafe interactions, complicating moderation efforts and highlighting the need for two-way safety systems.</p>
    <div class="responsive-chart">
        <div class="desktop-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/risk_sources_desktop.svg" type="image/svg+xml"></object>
        </div>
        <div class="mobile-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/risk_sources_mobile.svg" type="image/svg+xml"></object>
        </div>
        <p class="chart-caption">The LLM generates profanity about twice as often as users, likely due to its exposure to profane language during training and its attempts to mimic conversational tone.</p>
    </div>
    <p>
        Policy-violating content in LLM conversations originates from both users and the AI, with some risks introduced directly by the model itself. For example, “Other Risks” was almost evenly split between user prompts and LLM responses. In categories like profanity, the LLM was flagged for introducing offensive language nearly twice as often as users.
    </p>
</section>

<!-- Chart 3: Multi-Risk Bar Chart -->
<section class="container">
    <h2>Complex Moderation: Multiple Risks in a Single Conversation</h2>
    <p>While most unsafe content involves a single risk category, hundreds of conversations contain multiple overlapping risks-creating complex challenges for AI moderation systems.</p>
    <div class="responsive-chart">
        <div class="desktop-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/multi_risk_desktop.svg" type="image/svg+xml"></object>
        </div>
        <div class="mobile-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/multi_risk_mobile.svg" type="image/svg+xml"></object>
        </div>
        <p class="chart-caption">Most flagged conversations involve a single risk, but a significant minority involve two or more, increasing moderation complexity.</p>
    </div>
    <p>
        While the majority of flagged conversations (over 9,000) involve just one type of risk, a significant minority (more than 1,400) contain multiple risk categories. These multi-risk conversations represent the most complex cases, requiring more sophisticated moderation approaches that can understand nuanced combinations of harmful content.
    </p>
</section>

<footer>
    GitHub Repo: <a href="https://github.com/lg3394/trust-safety-ai-analysis"></a> |
    Analysis by Lucia de la Torre |
    Published May 5, 2025
</footer>
</body>
</html>
