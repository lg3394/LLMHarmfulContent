<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Content Safety: Analysis of Risk Categories in LLM Interactions</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px auto;
            max-width: 1200px;
            background-color: #ffffff;
        }

        h1, h2 {
            text-align: center;
            color: #333;
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        h2 {
            font-size: 1.8em;
            margin-top: 20px;
        }

        p {
            margin-bottom: 20px;
            color: #555;
        }

        .byline {
            text-align: center;
            font-style: italic;
            margin-bottom: 20px;
        }

        .container {
            margin-bottom: 40px;
        }

        .responsive-chart img, .responsive-chart object {
            max-width: 100%;
            height: auto;
            margin: 0 auto;
            display: block;
        }

        .chart-caption {
            text-align: center;
            font-style: italic;
            margin-top: 10px;
            color: #666;
        }

        @media (max-width: 768px) {
            .desktop-version { display: none; }
            .mobile-version { display: block; }
        }

        @media (min-width: 769px) {
            .desktop-version { display: block; }
            .mobile-version { display: none; }
        }

        .side-by-side {
            display: flex;
            gap: 20px;
        }

        @media (max-width: 768px) {
            .side-by-side {
                flex-direction: column;
                gap: 10px;
            }
        }

        .side-by-side > div {
            flex: 1;
        }

        footer {
            text-align: center;
            color: #888;
            margin-top: 40px;
            padding: 20px;
            border-top: 1px solid #eee;
        }
    </style>
</head>
<body>

    <!-- Title and Subtitle -->
    <header>
        <h1>How Safe Is AI? Inside the Risks of Large Language Models</h1>
        <h2>Analyzing Content Safety Risks in AI Conversations</h2>
        <p class="byline">By Lucia de la Torre</p>
    </header>

    <!-- Introduction -->
    <section class="container">
        <p>As artificial intelligence systems like ChatGPT and Mistral-7B become ever more embedded in our daily lives, the question of how to keep these models safe from generating or spreading harmful content has never been more urgent. But what, exactly, are the most common risks in AI-generated conversations? And how well can current moderation systems-and the models themselves-keep up?</p>
        
        <p>To answer these questions, I analyzed the Aegis AI Content Safety Dataset, a collection of roughly 11,000 conversations between humans and a large language model, all meticulously annotated by trained experts for a wide range of safety risks. By merging these annotations with a modern taxonomy of online harms, I set out to visualize and quantify the real risks that arise when people interact with powerful AI.</p>
    </section>

    <!-- Chart 1: Most Common Content Risks -->
    <section class="container">
        <h2>Most Common Content Risks in LLM Interactions</h2>
        <p>The most frequently identified types of unsafe or policy-violating content based on human review of 11,000 conversations between users and large language model do not fall under a single category.</p>
        
        <div class="responsive-chart">
            <!-- Desktop version -->
            <div class="desktop-version">
                <img src="risk_categories_desktop.svg" alt="Bar chart showing the distribution of content risks in LLM interactions, with Other Risks being the highest at 5,899, followed by Criminal Planning at 2,128, and Hate Speech at 1,078." width="800">
            </div>
            
            <!-- Mobile version -->
            <div class="mobile-version">
                <img src="risk_categories_mobile.svg" alt="Bar chart showing the distribution of content risks in LLM interactions, optimized for mobile view." width="320">
            </div>
            
            <p class="chart-caption">Nearly half of all flagged conversations were categorized as "Other Risks," highlighting the challenge of classifying harmful content.</p>
        </div>
        
        <p>The most striking finding: nearly half of all flagged conversations fell into the ambiguous "Other Risks" category-content that annotators found unsafe, but that didn't fit neatly into any predefined box. This ambiguity poses a major challenge for both AI and human moderators, as it's much harder to train algorithms to detect risks that defy clear definition.</p>
    </section>

    <!-- Chart 2: Risk Sources -->
    <section class="container">
        <h2>Who Introduces Risk: Users or the LLM?</h2>
        <p>A breakdown of unsafe content</p>
        
        <div class="responsive-chart">
            <!-- Desktop version -->
            <div class="desktop-version">
                <img src="risk_sources_desktop.svg" alt="Grouped bar chart showing risk sources by category, comparing LLM Response and User Prompt counts." width="800">
            </div>
            
            <!-- Mobile version -->
            <div class="mobile-version">
                <img src="risk_sources_mobile.svg" alt="Grouped bar chart showing risk sources by category, optimized for mobile view." width="320">
            </div>
            
            <p class="chart-caption">The LLM generates profanity about twice as often as users, likely due to its exposure to profane language during training and its attempts to mimic conversational tone.</p>
        </div>
        
        <p>Policy-violating content in LLM conversations originates from both users and the AI, with some risks introduced directly by the model itself. For example, "Other Risks" was almost evenly split between user prompts and LLM responses. In categories like profanity, the LLM was flagged for introducing offensive language nearly twice as often as users.</p>
    </section>

    <!-- Chart 3: Risk Co-occurrence -->
    <section class="container">
        <h2>Overlapping Risks: When Harmful Behaviors Appear Together</h2>
        <p>Many AI conversations include more than one type of unsafe content, revealing how harmful behaviors often intersect and complicate moderation.</p>
        
        <div class="responsive-chart">
            <!-- Desktop version -->
            <div class="desktop-version">
                <img src="risk_cooccurrence_600.svg" alt="Heatmap showing co-occurrence of risk categories in LLM conversations." width="600">
            </div>
            
            <!-- Mobile version -->
            <div class="mobile-version">
                <img src="risk_cooccurrence_320.svg" alt="Heatmap showing co-occurrence of risk categories, optimized for mobile view." width="320">
            </div>
            
            <p class="chart-caption">The heatmap reveals which types of harmful content frequently appear together in the same conversation.</p>
        </div>
        
        <p>This visualization helps us understand how different types of harmful content often overlap in the same conversation. This complexity makes moderation more challenging, as systems must recognize the interplay between different risk categories rather than treating each in isolation.</p>
    </section>

    <!-- Chart 4: Number of Risks per Conversation -->
    <section class="container">
        <h2>Most Harmful LLM Conversations Involve a Single Risk</h2>
        <p>While most flagged conversations contain only one type of unsafe content, hundreds involve multiple overlapping risks-highlighting that moderation systems must handle both straightforward and complex cases.</p>
        
        <div class="responsive-chart">
            <table style="width:100%; border-collapse: collapse; text-align: center; margin: 20px 0;">
                <thead>
                    <tr style="background-color: #f2f2f2;">
                        <th style="padding: 10px; border: 1px solid #ddd;">Number of Risk Categories</th>
                        <th style="padding: 10px; border: 1px solid #ddd;">Number of Conversations</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;">1</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">9,032</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;">2</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">1,223</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;">3</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">185</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;">4</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">19</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;">5</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">3</td>
                    </tr>
                </tbody>
            </table>
            
            <p class="chart-caption">Overlapping risks create more complex moderation challenges as they require understanding the interplay between different forms of harmful content.</p>
        </div>
        
        <p>While the majority of flagged conversations (over 9,000) involve just one type of risk, a significant minority (more than 1,400) contain multiple risk categories. These multi-risk conversations represent the most complex cases, requiring more sophisticated moderation approaches that can understand nuanced combinations of harmful content.</p>
    </section>
    
    <!-- Conclusion -->
    <section class="container">
        <h2>Why This Matters</h2>
        <p>The high rate of ambiguous "Other Risks" reveals a fundamental challenge in AI safety: many harmful or policy-violating interactions simply don't fit existing categories. This makes it difficult to train AI models to recognize them, and forces human moderators to make tough judgment calls-slowing down moderation and increasing the risk of inconsistency.</p>
        
        <p>As new forms of online harm emerge, taxonomies and moderation systems must continually evolve. The data suggests that AI alone cannot solve these challenges; ongoing human oversight and flexible, adaptive frameworks are essential.</p>
    </section>

    <!-- Footer -->
    <footer>
        <p>GitHub Repo: <a href="https://github.com/lg3394/trust-safety-ai-analysis/tree/main">Aegis AI Content Safety Dataset</a> | 
        Analysis by Lucia de la Torre | 
        Date Published: May 5, 2025</p>
    </footer>

</body>
</html>
