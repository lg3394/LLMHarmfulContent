<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How Safe Is AI? When LLMs Generate Harmful Content</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0 auto;
            max-width: 900px;
            background-color: #fff;
            padding: 16px;
        }
        h1, h2 {
            text-align: center;
            color: #222;
        }
        h1 { font-size: 2.2em; margin-bottom: 0.2em; }
        h2 { font-size: 1.4em; margin-top: 2em; }
        .byline { text-align: center; font-style: italic; margin-bottom: 20px; }
        .container { margin-bottom: 40px; }
        .responsive-chart object { max-width: 100%; height: auto; display: block; margin: 0 auto; }
        .chart-caption { text-align: center; font-style: italic; margin-top: 8px; color: #666; }
        @media (max-width: 600px) {
            .desktop-version { display: none; }
            .mobile-version { display: block; }
        }
        @media (min-width: 601px) {
            .desktop-version { display: block; }
            .mobile-version { display: none; }
        }
        footer {
            text-align: center;
            color: #888;
            margin-top: 40px;
            padding: 20px;
            border-top: 1px solid #eee;
        }
        .methodology {
            background-color: #f9f9f9;
            border: 1px solid #e0e0e0;
            padding: 20px;
            margin-top: 40px;
            border-radius: 5px;
        }
        .methodology h2 {
            margin-top: 0;
            color: #333;
        }
        .methodology ol {
            padding-left: 20px;
        }
        .methodology li {
            margin-bottom: 10px;
        }
    </style>
</head>
<body>

<header>
    <h1>How Safe Is AI? When LLMs Generate Harmful Content</h1>
    <h2>Even with advanced safeguards, moderating AI-generated conversations remains a complex challenge-ambiguous risks, overlapping harms, and both user and model behavior make it difficult to reliably detect and prevent unsafe content.</h2>
    <p class="byline">By Lucia de la Torre</p>
</header>

<section class="container">
    <p>
        As AI systems like ChatGPT and Mistral-7B become increasingly embedded in our lives, keeping these models safe from generating or spreading harmful content is more urgent than ever. But what kinds of risks are most common in AI-generated conversations? And why is content moderation still so hard?
    </p>
    <p>
        To answer these questions, I analyzed the <a href="https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-1.0">Aegis AI Content Safety Dataset</a>, a collection of about 11,000 conversations between humans and a large language model, all annotated by experts for a range of safety risks. By merging these annotations with a modern taxonomy of online harms, I set out to visualize and quantify the real risks that arise when people interact with powerful AI.
    </p>
    <p>
        As Facebook whistleblower Frances Haugen told Business Insider in 2022, "Artificial intelligence is nowhere near good enough to address problems facing content moderation on Facebook," after revealing internal documents showing the limits of automated systems at Meta.
    </p>
</section>

<section class="container">
    <h2>Inside the Dataset: How the Data Was Built</h2>
    <p>
        The Aegis AI Content Safety Dataset, curated by Nvidia, is one of the most comprehensive resources for studying AI safety in real-world settings. It's built on top of human preference data from Anthropic's HH-RLHF dataset, using only the prompts and then generating responses from Mistral-7B. The result: a trove of user prompts, LLM responses, and full dialogues, some benign, some deeply problematic.
    </p>
    <p>
        Annotation was performed by a team of twelve annotators and two data quality leads, all based in the United States and from diverse backgrounds. Each sample was reviewed for a broad taxonomy of risks-hate speech, violence, harassment, privacy violations, criminal planning, sexual content, profanity, and more. Importantly, annotators could also flag content as "Other Risks" for ambiguous or emerging harms not covered by existing categories.
    </p>
    <p>
        Quality assurance was rigorous: multiple annotators reviewed each sample, and leads regularly audited batches for consistency. Annotators were trained to self-correct and participated in regular group training to ensure guidelines were followed and edge cases were discussed. This process is designed to minimize bias and maximize the reliability of the resulting dataset<sup><a href="https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-1.0">[11]</a></sup>.
    </p>
</section>

<!-- Chart 1: Most Common Content Risks -->
<section class="container">
    <h2>The Moderation Challenge: Most Common Content Risks in LLM Interactions</h2>
    <p>Nearly half of flagged content doesn't fit standard risk categories, revealing a significant gap in current content moderation frameworks.</p>
    <div class="responsive-chart">
        <div class="desktop-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/most-common-harmful-content-landscape.svg" type="image/svg+xml"></object>
        </div>
        <div class="mobile-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/most-common-harmful-content-portrait.svg" type="image/svg+xml"></object>
        </div>
        <p class="chart-caption">Nearly half of all flagged conversations were categorized as "Other Risks," highlighting the challenge of classifying harmful content.</p>
    </div>
    <p>
        The most striking finding: nearly half of all flagged conversations fell into the ambiguous "Other Risks" category-content that annotators found unsafe, but that didn't fit neatly into any predefined box. This ambiguity poses a major challenge for both AI and human moderators, as it's much harder to train algorithms to detect risks that defy clear definition.
    </p>
</section>

<!-- Chart 2: Who Introduces Risk -->
<section class="container">
    <h2>Shared Responsibility: Who Introduces Harmful Content?</h2>
    <p>Both users and AI models contribute to unsafe interactions, complicating moderation efforts and highlighting the need for two-way safety systems.</p>
    <div class="responsive-chart">
        <div class="desktop-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/who-introduces-risk-landscape.svg" type="image/svg+xml"></object>
        </div>
        <div class="mobile-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/who-introduces-risk-portrait.svg" type="image/svg+xml"></object>
        </div>
        <p class="chart-caption">The LLM generates profanity about twice as often as users, likely due to its exposure to profane language during training and its attempts to mimic conversational tone.</p>
    </div>
    <p>
        Policy-violating content in LLM conversations originates from both users and the AI, with some risks introduced directly by the model itself. For example, "Other Risks" was almost evenly split between user prompts and LLM responses. In categories like profanity, the LLM was flagged for introducing offensive language nearly twice as often as users.
    </p>
</section>

<!-- Chart 3: Multi-Risk Bar Chart -->
<section class="container">
    <h2>Complex Moderation: Multiple Risks in a Single Conversation</h2>
    <p>While most unsafe content involves a single risk category, hundreds of conversations contain multiple overlapping risks-creating complex challenges for AI moderation systems.</p>
    <div class="responsive-chart">
        <div class="desktop-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/single_risk_landscape.svg" type="image/svg+xml"></object>
        </div>
        <div class="mobile-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/single_risk_portrait.svg" type="image/svg+xml"></object>
        </div>
        <p class="chart-caption">Most flagged conversations involve a single risk, but a significant minority involve two or more, increasing moderation complexity.</p>
    </div>
    <p>
        While the majority of flagged conversations (over 9,000) involve just one type of risk, a significant minority (more than 1,400) contain multiple risk categories. These multi-risk conversations represent the most complex cases, requiring more sophisticated moderation approaches that can understand nuanced combinations of harmful content.
    </p>
</section>

<!-- Chart 4: Ambiguous Risks Pie Chart -->
<section class="container">
    <h2>Ambiguous Risks Pose a Challenge</h2>
    <p>Nearly half of all flagged content in LLM conversations is too ambiguous to fit a set category, complicating efforts to detect and moderate harmful material.</p>
    <div class="responsive-chart">
        <div class="desktop-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/ambiguous_content_landscape.svg" type="image/svg+xml"></object>
        </div>
        <div class="mobile-version">
            <object data="https://lg3394.github.io/trust-safety-ai-analysis/svg/ambiguous_content_portrait.svg" type="image/svg+xml"></object>
        </div>
        <p class="chart-caption">Ambiguous risks make up a large share of flagged content, highlighting the limits of current moderation taxonomies.</p>
    </div>
    <p>
        When a large share of unsafe content is ambiguous, it exposes the limits of automated moderation and the ongoing need for human oversight, clearer guidelines, and adaptive systems.
    </p>
</section>

<section class="container">
    <h2>The Bigger Picture: Why Content Moderation Remains Hard</h2>
    <p>
        The findings from this dataset echo what many in the industry have warned: content moderation is not just a technical challenge, but a moving target. As LLMs become more capable, users find new ways to elicit harmful or policy-violating content, and new forms of risk emerge that don't fit existing categories. Even the best automated systems struggle with ambiguity, context, and cultural nuance. Human reviewers are essential, but the scale and speed of AI-generated content make manual review alone impractical.
    </p>
    <p>
        Merging and maintaining high-quality datasets is itself a challenge. As noted in industry research, combining multiple sources requires careful attention to label consistency, data quality, and the risk of bias or duplication<sup><a href="https://keylabs.ai/blog/merging-multiple-labeled-datasets-ensuring-consistency-and-quality/">[12]</a></sup>. The Aegis dataset's rigorous annotation and audit process is a model for future work, but it also shows the immense effort required to keep up with evolving harms.
    </p>
</section>

<section class="container">
    <h2>The Path Forward for Safer AI</h2>
    <p>
        The Aegis dataset offers a rare, transparent look at the real risks of AI-generated conversations. While most unsafe content falls into a handful of well-known categories, a substantial share remains ambiguous, defying easy classification. Both users and language models contribute to the problem, and overlapping risks make moderation even more complex.
    </p>
    <p>
        As AI continues to shape the digital world, understanding-and visualizing-these risks is a crucial step toward building safer, more trustworthy systems. The challenge now is to ensure our moderation tools, policies, and taxonomies keep pace with the evolving landscape of online harm. Collaboration between researchers, industry, and civil society will be essential to meet this challenge and to ensure that AI serves the public good.
    </p>
</section>

<!-- Methodology Section -->
<section class="container methodology">
    <h2>Methodology</h2>
    <p>
        This analysis explores patterns in AI safety risks by examining the <a href="https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-1.0">Aegis AI Content Safety Dataset</a>, a collection of approximately 11,000 human-LLM conversations annotated by Nvidia for various harmful content categories. The Aegis dataset itself builds upon Anthropic's HH-RLHF dataset prompts but generates new responses with Mistral-7B and includes expert annotations.
    </p>
    
    <p>
        My analysis went looks deeper into these datasets through several technical steps:
    </p>
    
    <ol>
        <li><strong>Taxonomy Analysis:</strong> I examined the annotation taxonomy used in the Aegis dataset and compared it with modern content moderation frameworks to understand how different types of harmful content are categorized. This provided context for interpreting the distribution of risk categories in the dataset.</li>
        
        <li><strong>Multi-dimensional Analysis:</strong> Using pivot tables and cross-tabulation techniques, I extracted complex relationships between risk categories to identify patterns of overlapping harms. This revealed that while most conversations contain a single risk (9,042), a significant number (1,485) contain multiple overlapping risks, adding complexity to moderation efforts.</li>
        
        <li><strong>Source Attribution Analysis:</strong> I analyzed the source of risk introduction (user vs. LLM) across different categories by extracting and comparing risk patterns from the dataset's annotations. This allowed me to quantify which party was responsible for introducing each risk category most frequently.</li>
        
        <li><strong>Ambiguity Quantification:</strong> I isolated and analyzed the "Other Risks" category that didn't fit standard taxonomies, revealing that 46% of all flagged content falls outside established classification systems. This highlighted a significant gap in current content moderation frameworks.</li>
        
        <li><strong>Data Transformation:</strong> I processed and restructured the datasets to enable effective visualization, creating derived datasets for each chart that highlighted different aspects of the content moderation challenge.</li>
    </ol>
    
    <p>
        All visualizations were created as responsive SVGs using ai2html to ensure proper display across desktop and mobile devices. I developed separate landscape and portrait versions of each chart to optimize the user experience on different screen sizes.
    </p>
    
    <p>
        <strong>Limitations:</strong> The dataset contains conversations with a specific model (Mistral-7B) and may not represent behavior across all LLMs. The annotations reflect judgments from Nvidia's team of U.S.-based annotators and may not capture cultural nuances across global contexts. The "Other Risks" category lacks detailed sub-categorization, limiting deeper analysis of these ambiguous harms.
    </p>
    
    <p>
        The complete code for this analysis, including data processing and visualization scripts, is available in the <a href="https://github.com/lg3394/trust-safety-ai-analysis">GitHub repository</a>.
    </p>
</section>

<footer>
    <a href="https://github.com/lg3394/trust-safety-ai-analysis">GitHub Repo</a> |
    Published 5 May 2025
</footer>
</body>
</html>
